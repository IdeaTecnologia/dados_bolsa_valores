name: Atualizar Ações Diariamente

on:
  schedule:
    - cron: '0 23 * * *'  # 20h no horário de Brasília (UTC-3)
  workflow_dispatch:      # Permite acionar manualmente

jobs:
  update:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      # Instala o Google Chrome, dependência para o SeleniumBase rodar
      - name: Install Google Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      # Instala as dependências do Python, incluindo o SeleniumBase
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      # Instala o chromedriver, que o SeleniumBase usará para controlar o Chrome
      - name: Install SeleniumBase browser drivers
        run: seleniumbase install chromedriver

      # Executa o script principal de scraping
      - name: Run Scraper
        run: python main.py

      # Faz o upload dos arquivos de debug (screenshot e html) para análise
      - name: Upload Artifacts for Debugging
        if: always() # Garante que este passo rode mesmo se o 'Run Scraper' falhar
        uses: actions/upload-artifact@v4
        with:
          name: scraper-debug-artifacts
          path: |
            *_screenshot.png
            *_pagina.html
      
      # Faz o commit e push do arquivo de dados atualizado
      - name: Commit and push changes
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add dados_acoes.json
          # Evita erro de commit se não houver mudanças no arquivo
          git diff-index --quiet HEAD || git commit -m "Atualização automática $(date "+%d/%m/%Y %H:%M")"
          git push